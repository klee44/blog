---
layout: post
title:  "VAE - variational autoencoder"
date:   2019-02-27 23:19:00 -0800
comments: true
categories: study  
---

In this post, I briefly introduce Variational Autoencoder (VAE). In the sequel, I am going to review details/improvements/variants of VAE.

* [Preliminaries](#prelim)
* [Variational autoencoder](#VAE)
	* [Problem scenario](#scenario)
	* [Maximum likelihood (ML) estimation and intractability](#MLestimation)
	* [The variational bound](#ELBO)
	* [Connections with autoencoder](#AE)
* [References](#ref)

## <a name="prelim"></a>  Preliminaries
Expection:
$$
\mathbb E_x \left[ f \right] = \int\, f(x)\, p(x) dx
$$

Kullback\\(--\\)Leibler (KL) divergence:
$$
D_{\text{KL}}(p||q) = \mathbb E_x \left[\log \frac{p(x)}{q(x)} \right]
$$


Bayes' rule:
$$
\begin{align*}
p(z|x) = \frac{p(x|z)p(z)}{p(x)} 
\end{align*}
$$


## <a name="VAE"></a> Variational Autoencoder 
### <a name="scenario"></a> Problem scenario - variational inference/latent variable model
Consider a latent variable model, where  \\(N\\) i.i.d. data samples, \\(X = \\{x^{(i)}\\}_{i=1}^{N}\\), are generated by some random process, involving an unobserved latent variable, \\(z\\). With the real parameter \\(\theta^\ast\\), the data generation process takes two steps: 

1. \\(z^{(i)}\\) is generated from a prior distribution \\(p_{\theta^\ast}(z)\\), 

2. \\(x^{(i)}\\) is generated from a conditional distribution \\(p_{\theta^\ast}(x \mid z = z^{(i)})\\).

But, we don't know what \\(\theta^\ast\\) or the values of the latent variable \\(z^{(i)}\\) are. 


### <a name="MLestimation"></a> Maximum likelihood (ML) estimation and intractability
A typical approach is to find a parameter \\(\theta\\) that maximizes the marginal likelihood:

$$
\theta^{\ast} = \underset{\theta}{\arg\max} \prod_{i=1}^{N} p_{\theta}(x^{(i)}),
$$

or, equivalently, log marginal likelihood,

$$
\theta^{\ast} = \underset{\theta}{\arg\max} \sum_{i=1}^{N} \log p_{\theta}(x^{(i)}).
$$

But, there are some cases where a direct ML estimation can be intractable, for example, 

* marginal likelihood: (evaluating the following integral can be intractable)

$$
\begin{align*}
p_{\theta}(x) = \color{red}{\int} p_{\theta}(z) p_{\theta}(x|z) dz,
\end{align*}
$$

* posteior density: 

$$
p_{\theta}(z\mid x) = \frac{p_{\theta}(x \mid z) p_{\theta}(z)}{\color{red}{p_{\theta}(x)}}.
$$


### <a name="ELBO"></a> The variational bound - evidence lower bound (ELBO)
Instead of working directly on intractable marginal likelihood, we can decompose and rewrite the marginal likelihood as 

$$
\log p_{\theta}(x^{(i)}) = D_{\text{KL}} \left(q_{\phi}(z | x^{(i)}) || p_{\theta}(z|x^{(i)})\right) + \mathcal L (\theta,\phi, x^{(i)})
$$

where the first term measures the distance of some approximate posterior \\(q_{\phi}(z\mid x^{(i)})\\) from the true posterior \\(p_{\theta}(z\mid x^{(i)})\\). Since the KL-divergence is non-negative, the second term is called the variational lower bound (or, evidence lower bound) 

$$
\begin{align}
\log p_{\theta}(x^{(i)}) \geq \mathcal L(\theta, \phi, x^{(i)})
\end{align}
$$

and can be written as

$$
\begin{align}
\mathcal L(\theta, \phi, x^{(i)}) &= \mathbb E_{q_{\phi}(z | x^{(i)})} \left[-\log q_{\phi}(z|x^{(i)}) + \log p_{\theta} (x^{(i)},z)  \right],\\
&= \mathbb E_{q_{\phi}(z | x^{(i)})} \left[\log p_{\theta} (x^{(i)}|z) \right] - D_{\text{KL}}\left(q_{\phi}(z|x^{(i)}||p_{\theta}(z)\right).
\end{align}
$$

Thus, maximizing ELBO with respect to \\(\phi, \theta\\) results in a recognition model \\(q_{\phi}(z \mid x) \\), which approximates the intractable true posterior \\(p_{\theta} (z \mid x)\\) and a generative model \\(p_{\theta}(x \mid z)\\).

The decomposition of the marginal likelihood can be verified as 

$$
\begin{align*}
\log p_{\theta}(x^{(i)}) &= \mathbb E_{q_{\phi}(z | x^{(i)})} \left[ \log p_{\theta}(x^{(i)}) \right]\\
&= \mathbb E_{q_{\phi}(z | x^{(i)})} \left[ \log \color{red}{p_{\theta}(x^{(i)})} \frac{ \color{blue}{p_\theta(z)}}{ \color{green}{p_\theta(z)}} \frac{ \color{green}{p_{\theta}(x^{(i)},z)}}{ \color{red}{p_{\theta}(x^{(i)},z)}} \frac{ \color{red}{q_{\phi}(z \mid x^{(i)})}}{\color{blue}{q_{\phi}(z \mid x^{(i)})}} \right]\\
&= \mathbb E_{q_{\phi}(z | x^{(i)})} \left[ \log \color{green}{p_{\theta} (x^{(i)}|z)} + \log \frac{ \color{blue}{p_{\theta}(z)}}{\color{blue}{q_{\phi}(z \mid x^{(i)})}}  + \log \frac{ \color{red}{q_{\phi}(z \mid x^{(i)})}}{ \color{red}{p_{\theta} (z\mid x^{(i)})}}  \right]\\
&= \mathbb E_{q_{\phi}(z | x^{(i)})} \left[\log p_{\theta} (x^{(i)}|z) \right] -D_{\text{KL}} \left(q_{\phi}(z \mid x^{(i)}) || p_{\theta}(z) \right) + D_{\text{KL}} \left(q_{\phi} (z \mid x^{(i)}) || p_{\theta} (z\mid x^{(i)}) \right)
\end{align*}
$$

### <a name="AE"></a> Connections with autoencoder
The ELBO reveals how variational inference can be connected with autoencoder. In the ELBO, the first term \\(\mathbb E_{q_{\phi}(z \mid x^{(i)})} \left[\log p_{\theta} (x^{(i)}\mid z) \right]\\) can be interpreted as __an expected reconstruction error__:

1. the latent variable \\(z\\) is inferred from the data via the recognition model \\( q_{\phi}(z \mid x^{(i)}) \\), and 

2. the reconstruction is constructed from the latent code via the generative model \\( p_{\theta} (x^{(i)}\mid z)\\). 

The recognition model and the generative model can be considered as a __stochastic encoder__ and a __stochastic decoder__, which model conditional probabilities parameterized by \\( \phi \\) and \\( \theta \\).

The second term in ELBO, \\(D_{\text{KL}}\left(q_{\phi}(z \mid x^{(i)} \vert\vert p_{\theta}(z)\right)\\), acts as __a regulerizer__, which measures the distance of the approximate posterior \\(q_{\phi}(z \mid x^{(i)})\\) from the prior \\(p_{\theta}(z)\\).


## <a name="ref"></a>  References
[1] Diederik P. Kingma, and Max Welling. ["Auto-encoding variational bayes.‚Äù](https://arxiv.org/abs/1312.6114) ICLR 2014.

[2] Danilo J. Rezende, Shakir Mohamed, Daan Wierstra. ["Stochastic Backpropagation and Approximate Inference in Deep Generative Models."](https://arxiv.org/abs/1401.4082) ICML 2014.
{% if page.comments %}
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://klee44-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
{% endif %}

