---
layout: post
title:  "VAE - variational autoencoder"
date:   2019-02-27 23:19:00 -0800
comments: true
categories: study  
---


* [Preliminaries](#prelim)
* [Variational autoencoder](#VAE)
	* [Problem scenario](#scenario)
	* [Maximum likelihood (ML) estimation and intractability](#MLestimation)
	* [The variational bound](#ELBO)
	* [Connections with autoencoder](#AE)

## <a name="prelim"></a>  Preliminaries
Expection:
$$
\mathbb E_x \left[ f \right] = \int\, f(x)\, p(x) dx
$$

Kullback\\(--\\)Leibler (KL) divergence:
$$
D_{\text{KL}}(p||q) = \mathbb E_x \left[\log \frac{p(x)}{q(x)} \right]
$$


Bayes' rule:
$$
\begin{align*}
p(z|x) = \frac{p(x|z)p(z)}{p(x)} 
\end{align*}
$$


## <a name="VAE"></a> Variational Autoencoder 
### <a name="scenario"></a> Problem scenario - variational inference/latent variable model
Consider a latent variable model, where  \\(N\\) i.i.d. data samples, \\(X = \\{x^{(i)}\\}_{i=1}^{N}\\), are generated by some random process, involving an unobserved latent variable, \\(z\\). With the real parameter \\(\theta^\ast\\), the data generation process takes two steps: 

1. \\(z^{(i)}\\) is generated from a prior distribution \\(p_{\theta^\ast}(z)\\), 

2. \\(x^{(i)}\\) is generated from a conditional distribution \\(p_{\theta^\ast}(x \mid z = z^{(i)})\\).

But, we don't know what \\(\theta^\ast\\) or the values of the latent variable \\(z^{(i)}\\) are. 


### <a name="MLestimation"></a> Maximum likelihood (ML) estimation and intractability
A typical approach is to find a parameter \\(\theta\\) that maximizes the marginal likelihood:

$$
\theta^{\ast} = \underset{\theta}{\arg\max} \prod_{i=1}^{N} p_{\theta}(x^{(i)}),
$$

or, equivalently, log marginal likelihood,

$$
\theta^{\ast} = \underset{\theta}{\arg\max} \sum_{i=1}^{N} \log p_{\theta}(x^{(i)}).
$$

But, there are some cases where a direct ML estimation can be intractable, for example, 

* marginal likelihood: (evaluating the following integral can be intractable)

$$
\begin{align*}
p_{\theta}(x) = \color{red}{\int} p_{\theta}(z) p_{\theta}(x|z) dz,
\end{align*}
$$

* posteior density: 

$$
p_{\theta}(z\mid x) = \frac{p_{\theta}(x \mid z) p_{\theta}(z)}{\color{red}{p_{\theta}(x)}}.
$$


### <a name="ELBO"></a> The variational bound - evidence lower bound (ELBO)
Instead of working directly on intractable marginal likelihood, we can decompose and rewrite the marginal likelihood as 

$$
\log p_{\theta}(x^{(i)}) = D_{\text{KL}} \left(q_{\phi}(z | x^{(i)}) || p_{\theta}(z|x^{(i)})\right) + \mathcal L (\theta,\phi, x^{(i)})
$$

where the first term measures the distance of some approximate posterior \\(q_{\phi}(z\mid x^{(i)})\\) from the true posterior \\(p_{\theta}(z\mid x^{(i)})\\). Since the KL-divergence is non-negative, the second term is called the variational lower bound (or, evidence lower bound) 

$$
\begin{align}
\log p_{\theta}(x^{(i)}) \geq \mathcal L(\theta, \phi, x^{(i)})
\end{align}
$$

and can be written as

$$
\begin{align}
\mathcal L(\theta, \phi, x^{(i)}) &= \mathbb E_{q_{\phi}(z | x^{(i)})} \left[-\log q_{\phi}(z|x^{(i)}) + \log p_{\theta} (x^{(i)},z)  \right],\\
&= \mathbb E_{q_{\phi}(z | x^{(i)})} \left[\log p_{\theta} (x^{(i)}|z) \right] - D_{\text{KL}}\left(q_{\phi}(z|x^{(i)}||p_{\theta}(z)\right).
\end{align}
$$

Thus, maximizing ELBO with respect to \\(\phi, \theta\\) results in a recognition model \\(q_{\phi}(z \mid x) \\), which approximates the intractable true posterior \\(p_{\theta} (z \mid x)\\) and a generative model \\(p_{\theta}(x \mid z)\\).

The decomposition of the marginal likelihood can be verified as 

$$
\begin{align*}
\log p_{\theta}(x) &= \mathbb E_{q_{\phi}(z | x^{(i)})} \left[ \log p_{\theta}(x) \right]\\
&= \mathbb E_{q_{\phi}(z | x^{(i)})} \left[ \log \color{red}{p_{\theta}(x)} \frac{ \color{blue}{p_\theta(z)}}{ \color{green}{p_\theta(z)}} \frac{ \color{green}{p_{\theta}(x^{(i)},z)}}{ \color{red}{p_{\theta}(x^{(i)},z)}} \right]\\
&= \mathbb E_{q_{\phi}(z | x^{(i)})} \left[ \log \frac{ \color{blue}{p_{\theta}(z)}}{ \color{red}{p_{\theta}(z\mid x^{(i)})}} +  \log \color{green}{p_{\theta} (x^{(i)}|z)} \right]\\
&= -D_{\text{KL}} \left(p_{\theta}(z|x^{(i)}) || p_{\theta}(z) \right)  + \mathbb E_{q_{\phi}(z | x^{(i)})} \left[\log p_{\theta} (x^{(i)}|z) \right].
\end{align*}
$$

### <a name="AE"></a> Connections with autoencoder
The ELBO reveals how variational inference can be connected with autoencoder. In the ELBO, the first term \\(\mathbb E_{q_{\phi}(z \mid x^{(i)})} \left[\log p_{\theta} (x^{(i)}\mid z) \right]\\) can be interpreted as __an expected reconstruction error__:

1. the latent variable \\(z\\) is inferred from the data via the recognition model \\( q_{\phi}(z \mid x^{(i)}) \\), and 

2. the reconstruction is constructed from the latent code via the generative model \\( p_{\theta} (x^{(i)}\mid z)\\). 

The recognition model and the generative model can be considered as a __stochastic encoder__ and a __stochastic decoder__, which model conditional probabilities parameterized by \\( \phi \\) and \\( \theta \\).

The second term in ELBO, \\(D_{\text{KL}}\left(q_{\phi}(z \mid x^{(i)} \vert\vert p_{\theta}(z)\right)\\), acts as __a regulerizer__, which measures the distance of the approximate posterior \\(q_{\phi}(z \mid x^{(i)})\\) from the prior \\(p_{\theta}(z)\\).


